{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c9995fd",
   "metadata": {},
   "source": [
    "# 단어 수준 텍스트 생성\n",
    "\n",
    "이 노트북은 아래 문서에 실려 있는 예제를 실습하면서 작성한 것입니다.\n",
    "\n",
    "* [How to Develop a Word-Level Neural Language Model and Use it to Generate Text](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/) - Machine Learning Mastery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3866b09",
   "metadata": {},
   "source": [
    "## 데이터 준비\n",
    "\n",
    "* [Download The Republic by Plato (republic.txt)](http://www.gutenberg.org/cache/epub/1497/pg1497.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519df69",
   "metadata": {},
   "source": [
    "### 텍스트 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87beb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿BOOK I.\n",
      "\n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in wh\n"
     ]
    }
   ],
   "source": [
    "def load_doc(filename):\n",
    "    with open(filename, mode='r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    return text\n",
    "\n",
    "in_filename = 'republic_clean.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408e57f",
   "metadata": {},
   "source": [
    "### 텍스트 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b701dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said', 'to', 'me']\n",
      "Total Tokens: 117341\n",
      "Unique Tokens: 7323\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def clean_doc(doc):\n",
    "    doc = doc.replace('--', ' ')\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9451a8c",
   "metadata": {},
   "source": [
    "### 정제된 텍스트 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca7cb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 117290\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab1574f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    with open(filename, mode='w', encoding='utf-8') as f:\n",
    "        f.write(data)\n",
    "        \n",
    "out_filename = 'republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dce2dd0",
   "metadata": {},
   "source": [
    "## 언어 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c109f0",
   "metadata": {},
   "source": [
    "### 훈련 데이터 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64e5431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted', 'i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with']\n"
     ]
    }
   ],
   "source": [
    "def load_doc(filename):\n",
    "    with open(filename, mode='r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    return text\n",
    "\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "print(lines[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e42198",
   "metadata": {},
   "source": [
    "### 훈련 데이터 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e15cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23265684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117290, 51)\n",
      "(117290, 7324)\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "print(sequences.shape)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3deb512",
   "metadata": {},
   "source": [
    "### 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf491abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            366200    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 100)           60400     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7324)              739724    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,256,824\n",
      "Trainable params: 1,256,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d5dab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "917/917 [==============================] - 315s 334ms/step - loss: 6.1356 - accuracy: 0.0739\n",
      "Epoch 2/10\n",
      "917/917 [==============================] - 302s 330ms/step - loss: 5.6744 - accuracy: 0.1078\n",
      "Epoch 3/10\n",
      "917/917 [==============================] - 332s 362ms/step - loss: 5.4224 - accuracy: 0.1317\n",
      "Epoch 4/10\n",
      "917/917 [==============================] - 319s 347ms/step - loss: 5.3092 - accuracy: 0.1422\n",
      "Epoch 5/10\n",
      "917/917 [==============================] - 323s 352ms/step - loss: 5.1925 - accuracy: 0.1505\n",
      "Epoch 6/10\n",
      "917/917 [==============================] - 329s 359ms/step - loss: 5.1077 - accuracy: 0.1563\n",
      "Epoch 7/10\n",
      "917/917 [==============================] - 321s 350ms/step - loss: 5.0324 - accuracy: 0.1604\n",
      "Epoch 8/10\n",
      "917/917 [==============================] - 324s 353ms/step - loss: 4.9590 - accuracy: 0.1648\n",
      "Epoch 9/10\n",
      "917/917 [==============================] - 340s 371ms/step - loss: 4.8993 - accuracy: 0.1683\n",
      "Epoch 10/10\n",
      "917/917 [==============================] - 314s 342ms/step - loss: 4.8299 - accuracy: 0.1725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21d55ee0fa0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939546e1",
   "metadata": {},
   "source": [
    "### 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfcff32d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dump\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# save the model to file\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# save the tokenizer\u001b[39;00m\n\u001b[0;32m      6\u001b[0m dump(tokenizer, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from pickle import dump\n",
    "\n",
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea025469",
   "metadata": {},
   "source": [
    "## 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde441fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "seq_length = len(lines[0].split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3388782",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b8c98",
   "metadata": {},
   "source": [
    "### 텍스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2b5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
